df_raw, Smoke_habits, Sport_habits,
results.subtittle = F,
subtitle = paste0("Fisher's exact test", ", p-value = ", ifelse(test_fisher$p.value < 0.001, "< 0.001", round(test_fisher$p.value, 3)))
)
?shapiro.test()
# ANOVA in R
install.packages("palmerpenguins")
library(palmerpenguins)
head(penguins)
glimpse(penguins)
summary(penguins)
summary(df)
df <- penguins %>% select(species, flipper_length_mm)
summary(df)
df %>% group_by(species) %>% summarise(mean = mean(flipper_length_mm),
sd = sd(flipper_length_mm),
p25 = quantile(flipper_length_mm, probs = 0.25),
p50 = quantile(flipper_length_mm, probs = 0.5),
p75 = quantile(flipper_length_mm, probs = 0.75),
)
df %>% group_by(species) %>% summarise(mean = mean(flipper_length_mm),
sd = sd(flipper_length_mm),
p25 = quantile(flipper_length_mm, probs = 0.25),
p50 = quantile(flipper_length_mm, probs = 0.5),
p75 = quantile(flipper_length_mm, probs = 0.75),
)
summary(df)
df %>% group_by(species) %>% summarise(mean = mean(flipper_length_mm),
sd = sd(flipper_length_mm),
p25 = quantile(flipper_length_mm, probs = 0.25, na.rm = T),
p50 = quantile(flipper_length_mm, probs = 0.5, na.rm = T),
p75 = quantile(flipper_length_mm, probs = 0.75, na.rm = T),
)
df %>% ggplot() + geom_boxplot(aes(x = species, y = flipper_length_mm))
df %>% group_by(species) %>% summarise(mean = mean(flipper_length_mm, na.rm = T),
sd = sd(flipper_length_mm, na.rm = T),
p25 = quantile(flipper_length_mm, probs = 0.25, na.rm = T),
p50 = quantile(flipper_length_mm, probs = 0.5, na.rm = T),
p75 = quantile(flipper_length_mm, probs = 0.75, na.rm = T),
)
df %>% ggplot() + geom_boxplot(aes(x = species, y = flipper_length_mm))
df %>% ggplot() +
aes(x = species, y = flipper_length_mm, color = species) +
geom_point() +
theme()
df %>% ggplot() +
aes(x = species, y = flipper_length_mm, color = species) +
geom_jitter() +
theme()
df %>% ggplot() + geom_boxplot(aes(x = species, y = flipper_length_mm))
?aov()
# Underlying assumptions of ANOVA
# 1 Variable type
# The dependent variable flipper_length_mm is a quantitative variable and the independent variable species is a qualitative one (with 3 levels corresponding to the 3 species). So we have a mix of the two types of variable and this assumption is met.
# 2 Independence
# Independence of the observations is assumed as data have been collected from a randomly selected portion of the population and measurements within and between the 3 samples are not related.
# The independence assumption is most often verified based on the design of the experiment and on the good control of experimental conditions, as it is the case here.
# 3 Normality
# Since the smallest sample size per group (i.e., per species) is 68, we have large samples. Therefore, we do not need to check normality. Normally, we would directly test the homogeneity of the variances without testing normality. However, for the sake of illustration, we act as if the sample sizes were small in order to illustrate what would need to be done in that case.
# Remember that normality of residuals can be tested visually via a histogram and a QQ-plot, and/or formally via a normality test (Shapiro-Wilk test for instance).
# Before checking the normality assumption, we first need to compute the ANOVA (more on that in this section). We then save the results in res_aov :
res_aov <- aov(flipper_length_mm ~ species, data = df)
res_aov
summary(res_aov)
ggplot(data = tibble(x = res_aov$residuals)) + geom_histogram(aes(x))
# qq-plot
library(car)
# qq-plot
install.packages('car')
# qq-plot
install.packages('car')
library(car)
qqPlot(res_aov$residuals)
qqPlot(res_aov$residuals, id = F)
hist(res_aov$residuals)
qqPlot(res_aov$residuals, id = F)
par(mfrow = c(1,2)) # combine plots
hist(res_aov$residuals)
qqPlot(res_aov$residuals, id = F)
# Still for the sake of illustration, we also now test the normality assumption via a normality test. You can use the Shapiro-Wilk test or the Kolmogorov-Smirnov test, among others. Remember that the null and alternative hypothesis are:
# H0: data come from a normal distribution
# H1 : data do not come from a normal distribution
# In R, we can test normality of the residuals with the Shapiro-Wilk test thanks to the shapiro.test() function:
shapiro.test(res_aov$residuals)
# Testing normality on all residuals or on the observations per group is equivalent, and will give similar results. Indeed, saying “The distribution of Y within each group is normally distributed” is the same as saying “The residuals are normally distributed.”
# Remember that residuals are the distance between the actual value of Y and the mean value of Y for a specific value of X, so the grouping variable is induced in the computation of the residuals.
# So in summary, in ANOVA you actually have two options for testing normality:
# - Checking normality separately for each group on the “raw” data (Y values)
# - Checking normality on all residuals (but not per group)
# In practice, you will see that it is often easier to just use the residuals and check them all together, especially if you have many groups or few observations per group.
# 4 Equality of variances - homogeneity
# Assuming residuals follow a normal distribution, it is now time to check whether the variances are equal across species or not. The result will have an impact on whether we use the ANOVA or the Welch ANOVA.
# This can again be verified visually—via a boxplot or dotplot—or more formally via a statistical test (Levene’s test, among others).
# boxplot
boxplot(flipper_length_mm ~ species)
# Testing normality on all residuals or on the observations per group is equivalent, and will give similar results. Indeed, saying “The distribution of Y within each group is normally distributed” is the same as saying “The residuals are normally distributed.”
# Remember that residuals are the distance between the actual value of Y and the mean value of Y for a specific value of X, so the grouping variable is induced in the computation of the residuals.
# So in summary, in ANOVA you actually have two options for testing normality:
# - Checking normality separately for each group on the “raw” data (Y values)
# - Checking normality on all residuals (but not per group)
# In practice, you will see that it is often easier to just use the residuals and check them all together, especially if you have many groups or few observations per group.
# 4 Equality of variances - homogeneity
# Assuming residuals follow a normal distribution, it is now time to check whether the variances are equal across species or not. The result will have an impact on whether we use the ANOVA or the Welch ANOVA.
# This can again be verified visually—via a boxplot or dotplot—or more formally via a statistical test (Levene’s test, among others).
# boxplot
boxplot(flipper_length_mm ~ species, data = df)
library(lattice)
dotplot(flipper_length_mm ~ species, data = df)
# Both the boxplot and the dotplot show a similar variance for the different species. In the boxplot, this can be seen by the fact that the boxes and the whiskers have a comparable size for all species. There are a couple of outliers as shown by the points outside the whiskers, but this does not change the fact that the dispersion is more or less the same between the different species.
# In the dotplot, this can be seen by the fact that points for all 3 species have more or less the same range, a sign of the dispersion and thus the variance being similar.
# Like the normality assumption, if you feel that the visual approach is not sufficient, you can formally test for equality of the variances with a Levene’s or Bartlett’s test. Notice that the Levene’s test is less sensitive to departures from normal distribution than the Bartlett’s test.
# The null and alternative hypothesis for both tests are:
# H0: variances are equal
# H1: at least one variance is different
# In R, the Levene’s test can be performed thanks to the leveneTest() function from the {car} package:
# Levene's test
library(car)
leveneTest(flipper_length_mm ~ species, data = df)
# The p-value being larger than the significance level of 0.05, we do not reject the null hypothesis, so we cannot reject the hypothesis that variances are equal between species (p-value = 0.719).
# This result is also in line with the visual approach, so the homogeneity of variances is met both visually and formally.
# Outliers
# There are several techniques to detect outliers. In this article, we focus on the most simple one (yet very efficient)—the visual approach via a boxplot:
boxplot(flipper_length_mm ~ species, data = df)
# The p-value being larger than the significance level of 0.05, we do not reject the null hypothesis, so we cannot reject the hypothesis that variances are equal between species (p-value = 0.719).
# This result is also in line with the visual approach, so the homogeneity of variances is met both visually and formally.
# Outliers
# There are several techniques to detect outliers. In this article, we focus on the most simple one (yet very efficient)—the visual approach via a boxplot:
par(mfrow = c(1,1))
boxplot(flipper_length_mm ~ species, data = df)
# ANOVA
# We showed that all assumptions of the ANOVA are met. We can thus proceed to the implementation of the ANOVA in R, but first, let’s do some preliminary analyses to better understand the research question.
# Preliminary analyses
# A good practice before actually performing the ANOVA in R is to visualize the data in relation to the research question. The best way to do so is to draw and compare boxplots of the quantitative variable flipper_length_mm for each species.
# This can be done with the boxplot() function in base R (same code than the visual check of equal variances):
boxplot(flipper_length_mm ~ species, data = df)
ggplot(df) + geom_boxplot(aes(x = species, y = flipper_length_mm))
# The boxplots above show that, at least for our sample, penguins of the species Gentoo seem to have the biggest flipper, and Adelie species the smallest flipper.
# Besides a boxplot for each species, it is also a good practice to compute some descriptive statistics such as the mean and standard deviation by species. This can be done, for instance, with the aggregate() function:
aggregate(flipper_length_mm ~ species, data = df, function(x) round(c(mean = mean(x), sd = sd(x))),2)
# The boxplots above show that, at least for our sample, penguins of the species Gentoo seem to have the biggest flipper, and Adelie species the smallest flipper.
# Besides a boxplot for each species, it is also a good practice to compute some descriptive statistics such as the mean and standard deviation by species. This can be done, for instance, with the aggregate() function:
aggregate(flipper_length_mm ~ species, data = df, function(x) round(c(mean = mean(x), sd = sd(x)),2)
# The boxplots above show that, at least for our sample, penguins of the species Gentoo seem to have the biggest flipper, and Adelie species the smallest flipper.
# Besides a boxplot for each species, it is also a good practice to compute some descriptive statistics such as the mean and standard deviation by species. This can be done, for instance, with the aggregate() function:
aggregate(flipper_length_mm ~ species, data = df, function(x) round(c(mean = mean(x), sd = sd(x)),2))
# The boxplots above show that, at least for our sample, penguins of the species Gentoo seem to have the biggest flipper, and Adelie species the smallest flipper.
# Besides a boxplot for each species, it is also a good practice to compute some descriptive statistics such as the mean and standard deviation by species. This can be done, for instance, with the aggregate() function:
aggregate(
flipper_length_mm ~ species,
data = df,
function(x) round(c(mean = mean(x), sd = sd(x)),2)
)
# or with summarise()
df %>% group_by(species) %>% summarise(mean = mean(flipper_length_mm, na.rm = T),
sd = sd(flipper_length_mm, na.rm = T))
# ANOVA in R
# As you guessed by now, only the ANOVA can help us to make inference about the population given the sample at hand, and help us to answer the initial research question “Is the length of the flippers different between the 3 species of penguins?”
# ANOVA in R can be done in several ways, of which two are presented below:
# 1 With the oneway.test() function:
oneway.test(flipper_length_mm ~ species, data = df, var.equal = T) # var.equal = T assumes equal variances
# 2 With the summary() and aov() functions:
res_aov <- aov(flipper_length_mm ~ species, data = df)
res_aov
summary(res_aov)
fit <- lm(flipper_length_mm ~ species, data = df)
fit
summary(fit)
hist(fit$residuals)
hist(res_aov$residuals)
# ANOVA in R
# As you guessed by now, only the ANOVA can help us to make inference about the population given the sample at hand, and help us to answer the initial research question “Is the length of the flippers different between the 3 species of penguins?”
# ANOVA in R can be done in several ways, of which two are presented below:
# 1 With the oneway.test() function:
oneway.test(flipper_length_mm ~ species, data = df, var.equal = T) # var.equal = T assumes equal variances
# 2 With the summary() and aov() functions:
res_aov <- aov(flipper_length_mm ~ species, data = df)
res_aov
summary(res_aov)
# As you can see from the two outputs above, the test statistic (F = in the first method and F value in the second one) and the p-value (p-value in the first method and Pr(>F) in the second one) are exactly the same for both methods, which means that in case of equal variances, results and conclusions will be unchanged.
# The advantage of the first method is that it is easy to switch from the ANOVA (used when variances are equal) to the Welch ANOVA (used when variances are unequal). This can be done by replacing var.equal = TRUE by var.equal = FALSE, as presented below:
oneway.test(flipper_length_mm ~ species, data = df, var.equal = F) # assuming not equal variances
# ANOVA in R
# As you guessed by now, only the ANOVA can help us to make inference about the population given the sample at hand, and help us to answer the initial research question “Is the length of the flippers different between the 3 species of penguins?”
# ANOVA in R can be done in several ways, of which two are presented below:
# 1 With the oneway.test() function:
oneway.test(flipper_length_mm ~ species, data = df, var.equal = T) # var.equal = T assumes equal variances
# The advantage of the second method, however, is that:
# the full ANOVA table (with degrees of freedom, mean squares, etc.) is printed, which may be of interest in some (theoritical) cases
# results of the ANOVA (res_aov) can be saved for later use (especially useful for post-hoc tests)
# Interpretations of ANOVA results
# Given that the p-value is smaller than 0.05, we reject the null hypothesis, so we reject the hypothesis that all means are equal. Therefore, we can conclude that at least one species is different than the others in terms of flippers length (p-value < 2.2e-16).
# (For the sake of illustration, if the p-value was larger than 0.05: we cannot reject the null hypothesis that all means are equal, so we cannot reject the hypothesis that the 3 considered species of penguins are equal in terms of flippers length.)
# A nice and easy way to report results of an ANOVA in R is with the report() function from the {report} package:
install.packages("remotes")
library(report)
remotes::install_github("easystats/report")
report(res_aov)
# The advantage of the second method, however, is that:
# the full ANOVA table (with degrees of freedom, mean squares, etc.) is printed, which may be of interest in some (theoritical) cases
# results of the ANOVA (res_aov) can be saved for later use (especially useful for post-hoc tests)
# Interpretations of ANOVA results
# Given that the p-value is smaller than 0.05, we reject the null hypothesis, so we reject the hypothesis that all means are equal. Therefore, we can conclude that at least one species is different than the others in terms of flippers length (p-value < 2.2e-16).
# (For the sake of illustration, if the p-value was larger than 0.05: we cannot reject the null hypothesis that all means are equal, so we cannot reject the hypothesis that the 3 considered species of penguins are equal in terms of flippers length.)
# A nice and easy way to report results of an ANOVA in R is with the report() function from the {report} package:
# install.packages("remotes")
# remotes::install_github("easystats/report") # You only need to do that once
library(report)
report(res_aov)
# Tukey HSD, used to compare all groups to each other (so all possible comparisons of 2 groups).
# Dunnett, used to make comparisons with a reference group. For example, consider 2 treatment groups and one control group. If you only want to compare the 2 treatment groups with respect to the control group, and you do not want to compare the 2 treatment groups to each other, the Dunnett’s test is preferred.
# Bonferroni correction if have a set of planned comparisons to do.
# The Bonferroni correction is simple: you simply divide the desired global
# α level by the number of comparisons. In our example, we have 3 comparisons so if we want to keep a global α = 0.05, we have α′ = 0.05/3 = 0.0167. We can then simply perform a Student’s t-test for each comparison, and compare the obtained p-values with this new α′.
# The other two post-hoc tests are presented in the next sections.
# Note that variances are assumed to be equal for all three methods (unless you use the Welch’s t-test instead of the Student’s t-test with the Bonferroni correction). If variances are not equal, you can use the Games-Howell test, among others.
# Tukey HSD test
# In our case, since there is no “reference” species and we are interested in comparing all species, we are going to use the Tukey HSD test.
# In R, the Tukey HSD test is done as follows. This is where the second method to perform the ANOVA comes handy because the results (res_aov) are reused for the post-hoc test:
install.packages("multcomp")
library(multcomp)
# Tukey HSD test
post_test <- glht(res_aov, linfct = mcp(species = "Tukey"))
# Tukey HSD test
tukey_test <- glht(res_aov, linfct = mcp(species = "Tukey"))
tukey_test
summary(tukey_test)
par(mar = c(3,8,3,3))
plot(tukey_test)
df %>% ggplot() + geom_boxplot(aes(x = species, y = flipper_length_mm))
plot(tukey_test)
# We see that the confidence intervals do not cross the zero line, which indicate that all groups are significantly different.
# Note that the Tukey HSD test can also be done in R with the TukeyHSD() function:
TukeyHSD(res_aov)
# With this code, it is the column p adj (also the last column) which is of interest. Notice that the conclusions are the same than above: all species are significantly different in terms of flippers length.
# The results can also be visualized with the plot() function:
plot(TukeyHSD(res_aov))
# Dunnett's test:
dunnett_test <- glht(res_aov, linfct = mcp(species = "Dunnett"))
dunnett_test
summary(dunnett_test)
# The interpretation is the same as for the Tukey HSD test’s except that in the Dunett’s test we only compare:
# - Chinstrap versus Adelie (line Chinstrap - Adelie == 0)
# - Gentoo vs. Adelie (line Gentoo - Adelie == 0)
# Both adjusted p-values (displayed in the last column) are below 0.05, so we reject the null hypothesis for both comparisons. This means that both the species Chinstrap and Gentoo are significantly different from the reference species Adelie in terms of flippers length. (Nothing can be said about the comparison between Chinstrap and Gentoo though.)
# Again, the results of the post-hoc test can be visualized with the plot() function:
par(mar = c(3,8,3,3))
plot(dunnett_test)
# The interpretation is the same as for the Tukey HSD test’s except that in the Dunett’s test we only compare:
# - Chinstrap versus Adelie (line Chinstrap - Adelie == 0)
# - Gentoo vs. Adelie (line Gentoo - Adelie == 0)
# Both adjusted p-values (displayed in the last column) are below 0.05, so we reject the null hypothesis for both comparisons. This means that both the species Chinstrap and Gentoo are significantly different from the reference species Adelie in terms of flippers length. (Nothing can be said about the comparison between Chinstrap and Gentoo though.)
# Again, the results of the post-hoc test can be visualized with the plot() function:
par(mar = c(0,0,0,0))
plot(dunnett_test)
# The interpretation is the same as for the Tukey HSD test’s except that in the Dunett’s test we only compare:
# - Chinstrap versus Adelie (line Chinstrap - Adelie == 0)
# - Gentoo vs. Adelie (line Gentoo - Adelie == 0)
# Both adjusted p-values (displayed in the last column) are below 0.05, so we reject the null hypothesis for both comparisons. This means that both the species Chinstrap and Gentoo are significantly different from the reference species Adelie in terms of flippers length. (Nothing can be said about the comparison between Chinstrap and Gentoo though.)
# Again, the results of the post-hoc test can be visualized with the plot() function:
par(mar = c(3,8,3,3))
plot(dunnett_test)
# The interpretation is the same as for the Tukey HSD test’s except that in the Dunett’s test we only compare:
# - Chinstrap versus Adelie (line Chinstrap - Adelie == 0)
# - Gentoo vs. Adelie (line Gentoo - Adelie == 0)
# Both adjusted p-values (displayed in the last column) are below 0.05, so we reject the null hypothesis for both comparisons. This means that both the species Chinstrap and Gentoo are significantly different from the reference species Adelie in terms of flippers length. (Nothing can be said about the comparison between Chinstrap and Gentoo though.)
# Again, the results of the post-hoc test can be visualized with the plot() function:
par(mar = c(3,3,3,3))
plot(dunnett_test)
# The interpretation is the same as for the Tukey HSD test’s except that in the Dunett’s test we only compare:
# - Chinstrap versus Adelie (line Chinstrap - Adelie == 0)
# - Gentoo vs. Adelie (line Gentoo - Adelie == 0)
# Both adjusted p-values (displayed in the last column) are below 0.05, so we reject the null hypothesis for both comparisons. This means that both the species Chinstrap and Gentoo are significantly different from the reference species Adelie in terms of flippers length. (Nothing can be said about the comparison between Chinstrap and Gentoo though.)
# Again, the results of the post-hoc test can be visualized with the plot() function:
par(mar = c(3,8,3,3))
plot(dunnett_test)
The reference category can be changed with the relevel() function (or with the {questionr} addin). Considering that we want Gentoo as the reference category instead of Adelie:
f
# We see that the confidence intervals do not cross the zero line, which indicate that both the species Gentoo and Chinstrap are significantly different from the reference species Adelie.
# Note that in R, by default, the reference category for a factor variable is the first category in alphabetical order. This is the reason that, by default, the reference species is Adelie.
# The reference category can be changed with the relevel() function (or with the {questionr} addin). Considering that we want Gentoo as the reference category instead of Adelie:
str(df)
# We see that the confidence intervals do not cross the zero line, which indicate that both the species Gentoo and Chinstrap are significantly different from the reference species Adelie.
# Note that in R, by default, the reference category for a factor variable is the first category in alphabetical order. This is the reason that, by default, the reference species is Adelie.
# The reference category can be changed with the relevel() function (or with the {questionr} addin). Considering that we want Gentoo as the reference category instead of Adelie:
summary(df)
# We see that the confidence intervals do not cross the zero line, which indicate that both the species Gentoo and Chinstrap are significantly different from the reference species Adelie.
# Note that in R, by default, the reference category for a factor variable is the first category in alphabetical order. This is the reason that, by default, the reference species is Adelie.
# The reference category can be changed with the relevel() function (or with the {questionr} addin). Considering that we want Gentoo as the reference category instead of Adelie:
glimpse(df)
# We see that the confidence intervals do not cross the zero line, which indicate that both the species Gentoo and Chinstrap are significantly different from the reference species Adelie.
# Note that in R, by default, the reference category for a factor variable is the first category in alphabetical order. This is the reason that, by default, the reference species is Adelie.
# The reference category can be changed with the relevel() function (or with the {questionr} addin). Considering that we want Gentoo as the reference category instead of Adelie:
str(df)
levels(df$species)
levels(df$species)
df$species <- relevel(df$species, ref = "Gentoo")
levels(df$species)
In order to perform the Dunnett’s test with the new reference we first need to rerun the ANOVA to take into account the new reference:
# Gentoo now being the first category of the three, it is indeed considered as the reference level.
# In order to perform the Dunnett’s test with the new reference we first need to rerun the ANOVA to take into account the new reference:
res_aov2 <- aov(flipper_length_mm ~ species, data = df)
summary(res_aov2)
res_aov2
# We can then run the Dunett’s test with the new results of the ANOVA:
dunnett_test2 <- glht(res_aov2, linfct = mcp(species = "Dunnett"))
summary(dunnett_test2)
par(mar = c(3,8,3,3))
plot(dunnett_test2)
# From the results above we conclude that Adelie and Chinstrap species are significantly different from Gentoo species in terms of flippers length (adjusted p-values < 1e-10).
# Note that even if your study does not have a reference group which you can compare to the other groups, it is still often better to do multiple comparisons determined by some research questions than to do all-pairwise tests. By reducing the number of post-hoc comparisons to what is necessary only, and no more, you maximize the statistical power.8
# Other p-values adjustment methods
# For the interested readers, note that you can use other p-values adjustment methods by using the pairwise.t.test() function:
pairwise.t.test(df$flipper_length_mm, df$species, p.adjust.method = "holm")
# By default, the Holm method is applied but other methods exist. See ?p.adjust for all available options.
?p.adjust
# Visualization of ANOVA and post-hoc tests on the same plot
library(ggstatsplot)
ggbetweenstats(
data = df,
x = species,
y = flipper_length_mm,
type = "parametric", # ANOVA or Kruskal-Wallis
var.equal = T, # ANOVA or Welch ANOVA
plot.type = 'box',
pairwise.comparisons = T,
pairwise.display = 'significant',
centrality.plotting = F,
bf.message = F
)
ggbetweenstats(
data = df,
x = species,
y = flipper_length_mm,
type = "parametric", # ANOVA or Kruskal-Wallis
var.equal = T, # ANOVA or Welch ANOVA
plot.type = 'box',
pairwise.comparisons = T,
pairwise.display = 'significant',
centrality.plotting = F,
bf.message = F
)
rbeta(100000,11,38)
hist(rbeta(100000,11,38))
?cor.test)
?cor.test())
?cor.test()
# Data
head(mtcars)
# The variables vs and am are categorical variables, so they are removed for this article:
df <- mtcars %>% select(-vs, -am)
# The variables vs and am are categorical variables, so they are removed for this article:
df <- mtcars %>% select(-vs, -am)
# The variables vs and am are categorical variables, so they are removed for this article:
df <- mtcars %>% select(vs, am)
# The variables vs and am are categorical variables, so they are removed for this article:
library(tidyverse)
df <- mtcars %>% select(vs, am)
df <- mtcars %>% select(-vs, -am)
head(df,5)
# Data
head(mtcars)
mtcars %>% select(wt)
typeof(mtcars)
class(mtcars)
df <- mtcars %>% select(vs, am)
df <- mtcars %>% select(mtcars, vs, am)
df <- mtcars %>% select(mtcars, vs, am)
# 3        2.35    4     1
# 4        2.09    1     4
# 5        1.87    2     4
# 6        1.95    3     4
# 7        2.08    2     2
# 8        2.01    3     2
# 9        1.84    4     2
# 10       2.06    1     3
# 11       1.97    2     3
# 12       2.22    4     3
cat_experiment %>% pivot_wider(names_from = color, values_from = creatinine)
# 3        2.35    4     1
# 4        2.09    1     4
# 5        1.87    2     4
# 6        1.95    3     4
# 7        2.08    2     2
# 8        2.01    3     2
# 9        1.84    4     2
# 10       2.06    1     3
# 11       1.97    2     3
# 12       2.22    4     3
library(tidyr)
cat_experiment %>% pivot_wider(names_from = color, values_from = creatinine)
# [1] 2
# Build the data.frame
creatinine <- c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22)
food <- as.factor(c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"))
color <- as.factor(rep(c("Black", "White", "Orange", "Spotted"), each = 3))
cat_experiment <- as.data.frame(cbind(creatinine, food, color))
cat_experiment
cat_experiment %>% pivot_wider(names_from = color, values_from = creatinine)
cat_experiment %>% pivot_wider(names_from = food, values_from = creatinine)
cat_experiment_1 <- tibble(
creatinine = c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22),
food = c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"),
color = rep(c("Black", "White", "Orange", "Spotted"), each = 3)
) %>% pivot_wider(names_from = food, values_from = creatinine)
cat_experiment_1 %>% pivot_wider(names_from = food, values_from = creatinine)
cat_experiment_1
cat_experiment_1 <- tibble(
creatinine = c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22),
food = c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"),
color = rep(c("Black", "White", "Orange", "Spotted"), each = 3)
)
cat_experiment_1
cat_experiment_1 %>% pivot_wider(names_from = food, values_from = creatinine)
food <- as.factor(c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"), levels = c("A", "B", "C"))
food <- as.factor(c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"), levels = c("A", "B", "C"))
# [1] 2
# Build the data.frame
creatinine <- c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22)
food <- as.factor(c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"), levels = c("A", "B", "C"))
color <- as.factor(rep(c("Black", "White", "Orange", "Spotted"), each = 3), levels = c("Black", "White", "Orange", "Spotted"))
cat_experiment <- as.data.frame(cbind(creatinine, food, color))
cat_experiment
?relevel()
food <- factor(c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"), levels = c("A", "B", "C"))
food
# [1] 2
# Build the data.frame
creatinine <- c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22)
food <- factor(c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"), levels = c("A", "B", "C", "D"))
color <- factor(rep(c("Black", "White", "Orange", "Spotted"), each = 3), levels = c("Black", "White", "Orange", "Spotted"))
cat_experiment <- as.data.frame(cbind(creatinine, food, color))
cat_experiment
food
color
cbind(creatinine, color, food)
# [1] 2
# Build the data.frame
creatinine <- c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22)
food <- as.factor(c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"))
color <- factor(rep(c("Black", "White", "Orange", "Spotted"), each = 3))
cat_experiment <- as.data.frame(cbind(creatinine, food, color))
cat_experiment
# 4        2.09    1     4
# 5        1.87    2     4
# 6        1.95    3     4
# 7        2.08    2     2
# 8        2.01    3     2
# 9        1.84    4     2
# 10       2.06    1     3
# 11       1.97    2     3
# 12       2.22    4     3
# AK: as pivot
library(tidyr)
cat_experiment_1 <- tibble(
creatinine = c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22),
food = c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"),
color = rep(c("Black", "White", "Orange", "Spotted"), each = 3)
)
cat_experiment_1 %>% pivot_wider(names_from = food, values_from = creatinine)
cat_experiment_1 <- tibble(
creatinine = c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22),
food = c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"),
color = rep(c("Black", "White", "Orange", "Spotted"), each = 3)
)
# 5        1.87    2     4
# 6        1.95    3     4
# 7        2.08    2     2
# 8        2.01    3     2
# 9        1.84    4     2
# 10       2.06    1     3
# 11       1.97    2     3
# 12       2.22    4     3
# AK: still don't understand: we don't have variant color = food = 2
# AK: as pivot, to save string names of variable
library(tidyr)
cat_experiment_1 <- tibble(
creatinine = c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22),
food = c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"),
color = rep(c("Black", "White", "Orange", "Spotted"), each = 3)
)
cat_experiment_1 %>% pivot_wider(names_from = food, values_from = creatinine)
?quantile()
?summarise_all
library(agricalae)
install.packages("agricolae")
library(agricalae)
?designs.lsd
?design.lsd
??design.lsd
?design.lsd
# 2 2017-01-01                   0
# 3 2017-01-01                   0
# 4 2017-01-01                   0
# 5 2017-01-01                   1
# 6 2017-01-01                   0
# 7 2017-01-01                   0
# 8 2017-01-01                   0
# 9 2017-01-01                   1
#10 2017-01-01                   0
# AK: so this is data per each user per day. Each day we have 10 users. 1 - she converted, 0 - no. No multiple conversions (we didn't have or decided to track only yes/no)
click_data %>% group_by(visit_date) %>% summarise(n = n(), sum = sum(clicked_adopt_today)) %>%
ggplot() + geom_line(aes(x = visit_date, y = n), color = 'red') + geom_line(aes(x = visit_date, y = sum))
?power.prop.test
?pwr.t.test()
install.packages("pwr")
library(pwr)
?pwr.t.test()
?t.test
letters
LETTERS
sample(LETTERS, 7)
concat()
